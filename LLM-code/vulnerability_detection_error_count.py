import json
import os
from collections import defaultdict, Counter

# Configuration
BANDIT_DIRS = [
    "LLM_Responses/bandit_reports/gemini",
    "LLM_Responses/bandit_reports/llama",
    "LLM_Responses/bandit_reports/wizardcoder",
    "LLM_Responses/bandit_reports/mistral",
    "LLM_Responses/bandit_reports/qwen",
    "LLM_Responses/bandit_reports/deepseekcoder",
    "LLM_Responses/bandit_reports/codellama",
    "LLM_Responses/bandit_reports/codegemma"
]
CODEQL_SARIF_PATH = "LLM_Responses/codeql_reports/all_results.sarif"
OUTPUT_DIR = "LLM_Responses/analysis_reports"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "llm_code_analysis_report.txt")

# CWE Description Database (abbreviated)
CWE_DESCRIPTIONS = {
    "CWE-79": "Improper Neutralization of Input During Web Page Generation (XSS)",
    "CWE-89": "SQL Injection",
    "CWE-119": "Buffer Overflow",
    "CWE-125": "Out-of-bounds Read",
    "CWE-20": "Improper Input Validation",
    "CWE-22": "Path Traversal",
    "CWE-78": "OS Command Injection",
    "CWE-416": "Use After Free",
    "CWE-190": "Integer Overflow",
    "CWE-352": "Cross-Site Request Forgery"
}

def analyze_bandit_reports():
    """Analyze Bandit reports for syntax/execution errors"""
    llm_counts = defaultdict(int)
    error_examples = defaultdict(list)
    error_types = Counter()

    for bandit_dir in BANDIT_DIRS:
        if not os.path.exists(bandit_dir):
            continue
        
        llm = os.path.basename(bandit_dir)
        
        for root, _, files in os.walk(bandit_dir):
            for file in files:
                if file.endswith('.json'):
                    filepath = os.path.join(root, file)
                    try:
                        with open(filepath) as f:
                            data = json.load(f)
                        
                        if data.get('errors'):
                            llm_counts[llm] += 1
                            
                            # Record error examples
                            if len(error_examples[llm]) < 3:
                                error_examples[llm].append({
                                    'file': os.path.basename(file),
                                    'errors': [e['reason'] for e in data['errors'][:2]]
                                })
                            
                            # Categorize error types
                            for error in data['errors']:
                                reason = error['reason'].lower()
                                if 'syntax' in reason:
                                    error_types['syntax'] += 1
                                elif 'import' in reason:
                                    error_types['import'] += 1
                                elif 'indent' in reason:
                                    error_types['indentation'] += 1
                                else:
                                    error_types['other'] += 1
                                    
                    except (json.JSONDecodeError, KeyError):
                        continue
    
    return dict(llm_counts), dict(error_types), error_examples

def analyze_codeql_reports():
    """Analyze CodeQL reports for security vulnerabilities"""
    with open(CODEQL_SARIF_PATH) as f:
        data = json.load(f)
    
    llm_counts = defaultdict(int)
    error_examples = defaultdict(list)
    cwe_stats = Counter()
    severity_counts = defaultdict(lambda: defaultdict(int))
    error_categories = defaultdict(lambda: defaultdict(int))
    rule_type_counts = defaultdict(lambda: defaultdict(int))
    rule_id_counts = defaultdict(lambda: Counter())
    rules_metadata = {}

    # Extract rule metadata
    for rule in data.get('runs', [{}])[0].get('tool', {}).get('driver', {}).get('rules', []):
        rules_metadata[rule['id']] = {
            'name': rule.get('name', 'N/A'),
            'description': rule.get('fullDescription', {}).get('text', 'N/A'),
            'severity': rule.get('properties', {}).get('problem.severity', 'N/A').lower(),
            'tags': rule.get('properties', {}).get('tags', []),
            'rule_type': rule['id'].split('/')[0] if '/' in rule['id'] else 'other'
        }
    
    # Process results
    for result in data.get('runs', [{}])[0].get('results', []):
        if not result.get('locations'):
            continue
            
        file_path = result['locations'][0]['physicalLocation']['artifactLocation']['uri']
        llm = file_path.split('/')[0]
        llm_counts[llm] += 1
        
        rule_id = result.get('ruleId', 'N/A')
        rule_meta = rules_metadata.get(rule_id, {})
        severity = rule_meta.get('severity', 'N/A')
        rule_type = rule_meta.get('rule_type', 'other')
        
        # Collect statistics
        severity_counts[llm][severity] += 1
        rule_type_counts[llm][rule_type] += 1
        rule_id_counts[llm][rule_id] += 1
        
        # Extract CWEs from tags
        cwes = []
        for tag in rule_meta.get('tags', []):
            if tag.startswith('external/cwe/cwe-'):
                cwe = tag.split('/')[-1].upper()
                cwes.append(cwe)
                cwe_stats[cwe] += 1
                error_categories[llm][cwe] += 1
        
        # Collect example error information
        if len(error_examples[llm]) < 3:
            error_info = {
                'rule_id': rule_id,
                'rule_name': rule_meta.get('name', 'N/A'),
                'message': result.get('message', {}).get('text', 'N/A'),
                'severity': severity,
                'cwe': cwes[0] if cwes else 'N/A',
                'file': os.path.basename(file_path),
                'description': rule_meta.get('description', 'N/A')
            }
            error_examples[llm].append(error_info)
    
    return (dict(llm_counts), error_examples, cwe_stats, 
            dict(severity_counts), dict(error_categories),
            dict(rule_type_counts), dict(rule_id_counts),
            rules_metadata)  # Include rules_metadata in the return

def generate_report(bandit_data, codeql_data):
    """Generate comprehensive analysis report"""
    bandit_counts, bandit_error_types, bandit_examples = bandit_data
    (codeql_counts, codeql_examples, cwe_stats, 
     severity_counts, error_categories,
     rule_type_counts, rule_id_counts,
     rules_metadata) = codeql_data  # Unpack rules_metadata
    
    report = []
    
    # Title and Summary
    report.append("LLM CODE GENERATION ANALYSIS REPORT")
    report.append("=" * 50)
    report.append("This report analyzes both:\n"
                "- Bandit: Syntax/execution errors (code quality)\n"
                "- CodeQL: Security vulnerabilities (code safety)")
    report.append("\nKey Metrics:")
    report.append("- Bandit errors indicate fundamentally broken code")
    report.append("- CodeQL findings reveal security risks in working code")
    report.append("- Together they measure both quality and safety")
    
    # Summary Table
    report.append("\n\nSUMMARY TABLE")
    report.append("-" * 60)
    report.append(f"{'LLM':<15} {'CodeQL Findings':>15} {'Bandit Errors':>15} {'Total':>15}")
    report.append("-" * 60)
    
    total_codeql = 0
    total_bandit = 0
    all_llms = set(codeql_counts.keys()).union(set(bandit_counts.keys()))
    
    for llm in sorted(all_llms):
        codeql = codeql_counts.get(llm, 0)
        bandit = bandit_counts.get(llm, 0)
        report.append(f"{llm:<15} {codeql:>15} {bandit:>15} {codeql+bandit:>15}")
        total_codeql += codeql
        total_bandit += bandit
    
    report.append("-" * 60)
    report.append(f"{'TOTAL':<15} {total_codeql:>15} {total_bandit:>15} {total_codeql+total_bandit:>15}")
    
    # Bandit Analysis
    report.append("\n\nBANDIT ERROR ANALYSIS (CODE QUALITY)")
    report.append("-" * 60)
    report.append(f"\nTotal files with errors: {total_bandit}")
    report.append("\nError Types Breakdown:")
    for err_type, count in sorted(bandit_error_types.items(), key=lambda x: -x[1]):
        report.append(f"- {err_type.capitalize()}: {count}")
    
    # CodeQL Analysis
    report.append("\n\nCODEQL VULNERABILITY ANALYSIS (CODE SAFETY)")
    report.append("-" * 60)
    report.append(f"\nTotal security findings: {total_codeql}")
    
    report.append("\nTop Vulnerability Types (CWEs):")
    for cwe, count in cwe_stats.most_common(10):
        desc = CWE_DESCRIPTIONS.get(cwe, "Security vulnerability")
        report.append(f"- {cwe}: {count} occurrences ({desc})")
    
    # Detailed LLM Breakdown
    report.append("\n\nDETAILED LLM PERFORMANCE")
    report.append("-" * 60)
    
    for llm in sorted(all_llms):
        report.append(f"\n{llm.upper()}")
        report.append("-" * 40)
        
        # Bandit Results
        if llm in bandit_counts:
            report.append(f"\nBandit Errors: {bandit_counts[llm]}")
            if bandit_counts[llm] > 0:
                report.append("Example Errors:")
                for example in bandit_examples.get(llm, [])[:3]:
                    report.append(f"- File: {example['file']}")
                    for error in example['errors'][:2]:
                        report.append(f"  â€¢ {error[:120]}{'...' if len(error) > 120 else ''}")
        
        # CodeQL Results
        if llm in codeql_counts:
            report.append(f"\nCodeQL Findings: {codeql_counts[llm]}")
            
            # Severity Breakdown
            report.append("\nBy Severity:")
            for severity, count in sorted(severity_counts[llm].items(), key=lambda x: x[0].lower() != 'error'):
                report.append(f"- {severity.capitalize()}: {count}")
            
            # Detailed Error-Type Breakdown
            if llm in rule_id_counts:
                report.append("\nDetailed Error Types:")
                for rule_id, count in rule_id_counts[llm].most_common():
                    rule_meta = rules_metadata.get(rule_id, {})
                    severity = rule_meta.get('severity', '').lower()
                    if severity == 'error':  # Only show error severity for this breakdown
                        rule_name = rule_meta.get('name', rule_id)
                        report.append(f"- {rule_id}: {count} ({rule_name})")
            
            # Top Vulnerability Types
            report.append("\nTop Vulnerability Types:")
            for cwe, count in sorted(error_categories[llm].items(), key=lambda x: -x[1])[:3]:
                desc = CWE_DESCRIPTIONS.get(cwe, "Security vulnerability")
                report.append(f"- {cwe}: {count} ({desc})")
            
            # Example Findings
            report.append("\nExample Findings:")
            for example in codeql_examples.get(llm, [])[:2]:
                report.append(f"- [{example['severity'].upper()}] {example['rule_id']} ({example['cwe']})")
                report.append(f"  Rule: {example['rule_name']}")
                report.append(f"  File: {example['file']}")
                report.append(f"  Description: {example['description'][:100]}...")
    
    # Recommendations
    report.append("\n\nRECOMMENDATIONS FOR RESEARCH REPORTING")
    report.append("-" * 60)
    report.append("1. Code Quality vs Security:\n"
                "   - Bandit errors measure code generation reliability\n"
                "   - CodeQL findings measure security consciousness")
    report.append("\n2. Key Metrics to Highlight:\n"
                "   - Total error rates by LLM\n"
                "   - Most common vulnerability types (CWEs)\n"
                "   - Specific error patterns (from detailed breakdown)")
    report.append("\n3. Comparative Analysis Suggestions:\n"
                "   - Which LLMs generate the most runnable code?\n"
                "   - Which produce the most secure code?\n"
                "   - Correlation between quality and security?")
    
    return "\n".join(report)

def main():
    print("Analyzing Bandit reports...")
    bandit_data = analyze_bandit_reports()
    
    print("Analyzing CodeQL reports...")
    codeql_data = analyze_codeql_reports()
    
    print("Generating report...")
    report = generate_report(bandit_data, codeql_data)
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    with open(OUTPUT_FILE, 'w') as f:
        f.write(report)
    
    print(f"\nAnalysis complete! Report saved to {OUTPUT_FILE}")
    print("\nHere's a quick summary:")
    print("\n".join(report.split("\n")[:30]))

if __name__ == "__main__":
    main()